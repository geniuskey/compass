{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability Demo\n",
    "\n",
    "This notebook demonstrates COMPASS's numerical stability features for RCWA simulations:\n",
    "\n",
    "1. Float32 vs float64 precision comparison\n",
    "2. Mixed precision eigendecomposition\n",
    "3. Adaptive fallback mechanism\n",
    "4. `StabilityDiagnostics` pre/post simulation checks\n",
    "\n",
    "RCWA is sensitive to numerical precision because it involves eigendecomposition\n",
    "of large complex matrices. Small floating-point errors can compound through the\n",
    "S-matrix recursion and produce unphysical results (QE > 1 or R + T + A != 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from compass.solvers.base import SolverFactory\n",
    "from compass.solvers.rcwa.stability import (\n",
    "    PrecisionManager,\n",
    "    AdaptivePrecisionRunner,\n",
    "    StabilityDiagnostics,\n",
    "    EigenvalueStabilizer,\n",
    ")\n",
    "from compass.runners.single_run import SingleRunner\n",
    "from compass.analysis.energy_balance import EnergyBalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Common Pixel Configuration\n",
    "\n",
    "We use a standard 2x2 BSI pixel. Stability issues are more likely\n",
    "with high Fourier orders and structures containing metallic layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"pixel\": {\n",
    "        \"pitch\": 1.0,\n",
    "        \"unit_cell\": [2, 2],\n",
    "        \"bayer_map\": [[\"R\", \"G\"], [\"G\", \"B\"]],\n",
    "        \"layers\": {\n",
    "            \"air\": {\"thickness\": 1.0, \"material\": \"air\"},\n",
    "            \"microlens\": {\n",
    "                \"enabled\": True, \"height\": 0.6,\n",
    "                \"radius_x\": 0.48, \"radius_y\": 0.48,\n",
    "                \"material\": \"polymer_n1p56\",\n",
    "                \"profile\": {\"type\": \"superellipse\", \"n\": 2.5, \"alpha\": 1.0},\n",
    "            },\n",
    "            \"planarization\": {\"thickness\": 0.3, \"material\": \"sio2\"},\n",
    "            \"color_filter\": {\n",
    "                \"thickness\": 0.6,\n",
    "                \"materials\": {\"R\": \"cf_red\", \"G\": \"cf_green\", \"B\": \"cf_blue\"},\n",
    "                \"grid\": {\"enabled\": True, \"width\": 0.05, \"material\": \"tungsten\"},\n",
    "            },\n",
    "            \"barl\": {\"layers\": [\n",
    "                {\"thickness\": 0.010, \"material\": \"sio2\"},\n",
    "                {\"thickness\": 0.025, \"material\": \"hfo2\"},\n",
    "            ]},\n",
    "            \"silicon\": {\n",
    "                \"thickness\": 3.0, \"material\": \"silicon\",\n",
    "                \"photodiode\": {\"position\": [0, 0, 0.5], \"size\": [0.7, 0.7, 2.0]},\n",
    "                \"dti\": {\"enabled\": True, \"width\": 0.1, \"material\": \"sio2\"},\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"source\": {\n",
    "        \"wavelength\": {\n",
    "            \"mode\": \"sweep\",\n",
    "            \"sweep\": {\"start\": 0.40, \"stop\": 0.70, \"step\": 0.01},\n",
    "        },\n",
    "        \"polarization\": \"unpolarized\",\n",
    "    },\n",
    "    \"compute\": {\"backend\": \"auto\"},\n",
    "}\n",
    "\n",
    "print(\"Base config: 2x2 BSI with tungsten grid (high-contrast)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Float32 vs Float64 Comparison\n",
    "\n",
    "Run the same simulation in pure float32 and pure float64 to see how\n",
    "precision affects QE accuracy and energy conservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Float32 configuration\n",
    "config_f32 = copy.deepcopy(base_config)\n",
    "config_f32[\"solver\"] = {\n",
    "    \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "    \"params\": {\"fourier_order\": [9, 9], \"dtype\": \"complex64\"},\n",
    "    \"stability\": {\n",
    "        \"precision_strategy\": \"float32\",\n",
    "        \"allow_tf32\": False,\n",
    "        \"fourier_factorization\": \"li_inverse\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Float64 configuration\n",
    "config_f64 = copy.deepcopy(base_config)\n",
    "config_f64[\"solver\"] = {\n",
    "    \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "    \"params\": {\"fourier_order\": [9, 9], \"dtype\": \"complex128\"},\n",
    "    \"stability\": {\n",
    "        \"precision_strategy\": \"float64\",\n",
    "        \"allow_tf32\": False,\n",
    "        \"fourier_factorization\": \"li_inverse\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Running float32...\")\n",
    "t0 = time.perf_counter()\n",
    "result_f32 = SingleRunner.run(config_f32)\n",
    "time_f32 = time.perf_counter() - t0\n",
    "\n",
    "print(\"Running float64...\")\n",
    "t0 = time.perf_counter()\n",
    "result_f64 = SingleRunner.run(config_f64)\n",
    "time_f64 = time.perf_counter() - t0\n",
    "\n",
    "print(f\"\\nFloat32: {time_f32:.2f}s\")\n",
    "print(f\"Float64: {time_f64:.2f}s\")\n",
    "print(f\"Speedup: {time_f64/time_f32:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare QE spectra\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), height_ratios=[3, 1])\n",
    "\n",
    "wl_nm = result_f64.wavelengths * 1000\n",
    "\n",
    "# Extract green pixel QE from both\n",
    "for name, qe in result_f32.qe_per_pixel.items():\n",
    "    if name.startswith(\"G\"):\n",
    "        ax1.plot(wl_nm, qe, \"--\", color=\"tab:blue\", alpha=0.7,\n",
    "                 label=f\"float32 ({name})\")\n",
    "        qe_f32_green = qe\n",
    "        break\n",
    "\n",
    "for name, qe in result_f64.qe_per_pixel.items():\n",
    "    if name.startswith(\"G\"):\n",
    "        ax1.plot(wl_nm, qe, \"-\", color=\"tab:orange\", linewidth=2,\n",
    "                 label=f\"float64 ({name})\")\n",
    "        qe_f64_green = qe\n",
    "        break\n",
    "\n",
    "ax1.set_ylabel(\"Green QE\")\n",
    "ax1.set_title(\"Float32 vs Float64: QE Comparison\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Difference plot\n",
    "diff = np.abs(np.array(qe_f32_green) - np.array(qe_f64_green))\n",
    "ax2.plot(wl_nm, diff, \"r-\", linewidth=1.5)\n",
    "ax2.axhline(0.01, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"1% threshold\")\n",
    "ax2.set_xlabel(\"Wavelength (nm)\")\n",
    "ax2.set_ylabel(\"|QE difference|\")\n",
    "ax2.set_title(\"Absolute QE Difference (f32 vs f64)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print(f\"Max |QE diff|: {np.max(diff):.5f}\")\n",
    "print(f\"Mean |QE diff|: {np.mean(diff):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixed Precision Eigendecomposition\n",
    "\n",
    "The `mixed` precision strategy runs most of the computation in float32 for speed,\n",
    "but promotes eigendecomposition to float64 (on CPU) for stability.\n",
    "\n",
    "This is the recommended default. Let's demonstrate it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate mixed precision eigendecomp on a synthetic matrix\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate a Fourier-domain matrix at order [9,9] -> 361 modes -> 722x722\n",
    "n = 722\n",
    "matrix_f32 = (np.random.randn(n, n) + 1j * np.random.randn(n, n)).astype(np.complex64)\n",
    "\n",
    "# Direct float32 eigendecomp\n",
    "t0 = time.perf_counter()\n",
    "evals_f32, evecs_f32 = np.linalg.eig(matrix_f32.astype(np.complex64))\n",
    "time_direct = time.perf_counter() - t0\n",
    "\n",
    "# Mixed precision eigendecomp (promotes to f64 internally)\n",
    "t0 = time.perf_counter()\n",
    "evals_mixed, evecs_mixed = PrecisionManager.mixed_precision_eigen(matrix_f32)\n",
    "time_mixed = time.perf_counter() - t0\n",
    "\n",
    "# Full float64 eigendecomp\n",
    "matrix_f64 = matrix_f32.astype(np.complex128)\n",
    "t0 = time.perf_counter()\n",
    "evals_f64, evecs_f64 = np.linalg.eig(matrix_f64)\n",
    "time_full = time.perf_counter() - t0\n",
    "\n",
    "print(f\"Direct float32: {time_direct:.3f}s\")\n",
    "print(f\"Mixed precision: {time_mixed:.3f}s\")\n",
    "print(f\"Full float64:   {time_full:.3f}s\")\n",
    "\n",
    "# Compare reconstruction error: ||A*v - lambda*v||\n",
    "def reconstruction_error(matrix, evals, evecs):\n",
    "    \"\"\"Compute eigendecomposition reconstruction error.\"\"\"\n",
    "    errors = []\n",
    "    for i in range(min(20, len(evals))):  # Check first 20\n",
    "        residual = matrix @ evecs[:, i] - evals[i] * evecs[:, i]\n",
    "        errors.append(np.linalg.norm(residual))\n",
    "    return np.mean(errors)\n",
    "\n",
    "err_f32 = reconstruction_error(matrix_f64, evals_f32.astype(np.complex128),\n",
    "                               evecs_f32.astype(np.complex128))\n",
    "err_mixed = reconstruction_error(matrix_f64, evals_mixed.astype(np.complex128),\n",
    "                                 evecs_mixed.astype(np.complex128))\n",
    "err_f64 = reconstruction_error(matrix_f64, evals_f64, evecs_f64)\n",
    "\n",
    "print(f\"\\nReconstruction error (||Av - lv||):\")\n",
    "print(f\"  Float32:         {err_f32:.6e}\")\n",
    "print(f\"  Mixed precision: {err_mixed:.6e}\")\n",
    "print(f\"  Float64:         {err_f64:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive Fallback Demonstration\n",
    "\n",
    "The `AdaptivePrecisionRunner` automatically escalates precision when\n",
    "energy conservation fails. The fallback chain is:\n",
    "\n",
    "1. GPU float32 (fastest)\n",
    "2. GPU float64 (if f32 fails energy check)\n",
    "3. CPU float64 (most stable, slowest)\n",
    "\n",
    "Here we demonstrate the mechanism by running it with a strict tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AdaptivePrecisionRunner with tight tolerance\n",
    "adaptive_runner = AdaptivePrecisionRunner(tolerance=0.005)  # 0.5% tolerance\n",
    "\n",
    "print(f\"Adaptive runner tolerance: {adaptive_runner.tolerance}\")\n",
    "print(f\"Fallback chain: GPU-f32 -> GPU-f64 -> CPU-f64\")\n",
    "print()\n",
    "\n",
    "# Simulate the fallback for a challenging wavelength\n",
    "# In practice, short wavelengths with metallic structures are hardest\n",
    "config_mixed = copy.deepcopy(base_config)\n",
    "config_mixed[\"solver\"] = {\n",
    "    \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "    \"params\": {\"fourier_order\": [9, 9]},\n",
    "    \"stability\": {\n",
    "        \"precision_strategy\": \"mixed\",\n",
    "        \"allow_tf32\": False,\n",
    "        \"fourier_factorization\": \"li_inverse\",\n",
    "        \"energy_check\": {\n",
    "            \"enabled\": True,\n",
    "            \"tolerance\": 0.005,\n",
    "            \"auto_retry_float64\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Running with mixed precision and auto-retry...\")\n",
    "result_mixed = SingleRunner.run(config_mixed)\n",
    "\n",
    "# Check energy balance\n",
    "if result_mixed.reflection is not None and result_mixed.transmission is not None:\n",
    "    total = np.array(result_mixed.reflection) + np.array(result_mixed.transmission)\n",
    "    if result_mixed.absorption is not None:\n",
    "        total += np.array(result_mixed.absorption)\n",
    "    max_error = np.max(np.abs(total - 1.0))\n",
    "    print(f\"Max energy error: {max_error:.6f}\")\n",
    "    print(f\"Energy check: {'PASS' if max_error < 0.01 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. StabilityDiagnostics: Pre-Simulation Check\n",
    "\n",
    "Before running a simulation, `StabilityDiagnostics.pre_simulation_check()`\n",
    "inspects the pixel stack and solver config for potential stability issues.\n",
    "\n",
    "It checks for:\n",
    "- Large Fourier order with insufficient precision\n",
    "- Thick layers that may cause S-matrix overflow\n",
    "- TF32 being enabled (catastrophic for RCWA)\n",
    "- Patterned layers with naive Fourier factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compass.materials.database import MaterialDB\n",
    "from compass.geometry.builder import GeometryBuilder\n",
    "\n",
    "# Build pixel stack for diagnostics\n",
    "mat_db = MaterialDB()\n",
    "builder = GeometryBuilder(base_config[\"pixel\"], mat_db)\n",
    "pixel_stack = builder.build()\n",
    "\n",
    "# Test 1: Good config (should have no warnings)\n",
    "good_config = {\n",
    "    \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "    \"params\": {\"fourier_order\": [9, 9]},\n",
    "    \"stability\": {\n",
    "        \"precision_strategy\": \"mixed\",\n",
    "        \"allow_tf32\": False,\n",
    "        \"fourier_factorization\": \"li_inverse\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"=== Pre-simulation check: Good config ===\")\n",
    "warnings = StabilityDiagnostics.pre_simulation_check(pixel_stack, good_config)\n",
    "if warnings:\n",
    "    for w in warnings:\n",
    "        print(f\"  WARNING: {w}\")\n",
    "else:\n",
    "    print(\"  No warnings. Config looks stable.\")\n",
    "\n",
    "# Test 2: Risky config (high order with float32, naive factorization)\n",
    "risky_config = {\n",
    "    \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "    \"params\": {\"fourier_order\": [17, 17]},\n",
    "    \"stability\": {\n",
    "        \"precision_strategy\": \"float32\",\n",
    "        \"fourier_factorization\": \"naive\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\n=== Pre-simulation check: Risky config ===\")\n",
    "warnings = StabilityDiagnostics.pre_simulation_check(pixel_stack, risky_config)\n",
    "for w in warnings:\n",
    "    print(f\"  WARNING: {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. StabilityDiagnostics: Post-Simulation Check\n",
    "\n",
    "After the simulation, `post_simulation_check()` validates that:\n",
    "- QE values are in the physical range [0, 1]\n",
    "- No NaN or Inf values in R, T, A\n",
    "- Energy conservation holds (R + T + A = 1 within tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-simulation check on our float64 result\n",
    "print(\"=== Post-simulation check: float64 result ===\")\n",
    "report_f64 = StabilityDiagnostics.post_simulation_check(result_f64)\n",
    "if report_f64:\n",
    "    for key, info in report_f64.items():\n",
    "        print(f\"  {key}: {info['status']} - {info['issue']}\")\n",
    "else:\n",
    "    print(\"  All checks passed. No issues detected.\")\n",
    "\n",
    "# Post-simulation check on our float32 result\n",
    "print(\"\\n=== Post-simulation check: float32 result ===\")\n",
    "report_f32 = StabilityDiagnostics.post_simulation_check(result_f32)\n",
    "if report_f32:\n",
    "    for key, info in report_f32.items():\n",
    "        print(f\"  {key}: {info['status']} - {info['issue']}\")\n",
    "else:\n",
    "    print(\"  All checks passed. No issues detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Energy Conservation vs Fourier Order\n",
    "\n",
    "Higher Fourier orders create larger matrices that are harder to solve\n",
    "stably. This plot shows how energy conservation error grows with order\n",
    "when using float32 vs mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = [5, 7, 9, 11, 13, 15]\n",
    "energy_err_f32 = []\n",
    "energy_err_mixed = []\n",
    "\n",
    "test_config = copy.deepcopy(base_config)\n",
    "test_config[\"source\"] = {\n",
    "    \"wavelength\": {\"mode\": \"single\", \"value\": 0.45},  # Blue: harder for stability\n",
    "    \"polarization\": \"unpolarized\",\n",
    "}\n",
    "\n",
    "for N in orders:\n",
    "    # Float32\n",
    "    cfg = copy.deepcopy(test_config)\n",
    "    cfg[\"solver\"] = {\n",
    "        \"name\": \"torcwa\", \"type\": \"rcwa\",\n",
    "        \"params\": {\"fourier_order\": [N, N], \"dtype\": \"complex64\"},\n",
    "        \"stability\": {\"precision_strategy\": \"float32\", \"allow_tf32\": False,\n",
    "                       \"fourier_factorization\": \"li_inverse\"},\n",
    "    }\n",
    "    r = SingleRunner.run(cfg)\n",
    "    if r.reflection is not None and r.transmission is not None:\n",
    "        total = float(r.reflection) + float(r.transmission)\n",
    "        if r.absorption is not None:\n",
    "            total += float(r.absorption)\n",
    "        energy_err_f32.append(abs(total - 1.0))\n",
    "    else:\n",
    "        energy_err_f32.append(np.nan)\n",
    "\n",
    "    # Mixed precision\n",
    "    cfg[\"solver\"][\"stability\"][\"precision_strategy\"] = \"mixed\"\n",
    "    r = SingleRunner.run(cfg)\n",
    "    if r.reflection is not None and r.transmission is not None:\n",
    "        total = float(r.reflection) + float(r.transmission)\n",
    "        if r.absorption is not None:\n",
    "            total += float(r.absorption)\n",
    "        energy_err_mixed.append(abs(total - 1.0))\n",
    "    else:\n",
    "        energy_err_mixed.append(np.nan)\n",
    "\n",
    "    print(f\"Order [{N},{N}]: f32 err={energy_err_f32[-1]:.6f}, \"\n",
    "          f\"mixed err={energy_err_mixed[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.semilogy(orders, energy_err_f32, \"o-\", label=\"Float32\",\n",
    "            color=\"tab:red\", linewidth=2, markersize=8)\n",
    "ax.semilogy(orders, energy_err_mixed, \"s-\", label=\"Mixed precision\",\n",
    "            color=\"tab:blue\", linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(0.01, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"1% threshold\")\n",
    "ax.axhline(0.02, color=\"gray\", linestyle=\":\", alpha=0.5, label=\"2% threshold\")\n",
    "\n",
    "ax.set_xlabel(\"Fourier Order N (matrix size = (2N+1)^2)\")\n",
    "ax.set_ylabel(\"|R + T + A - 1| (energy error)\")\n",
    "ax.set_title(\"Energy Conservation Error vs Fourier Order\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, which=\"both\")\n",
    "\n",
    "# Add matrix size on secondary x-axis\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(orders)\n",
    "ax2.set_xticklabels([(2*N+1)**2 for N in orders])\n",
    "ax2.set_xlabel(\"Matrix size (modes)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the COMPASS numerical stability toolkit:\n",
    "\n",
    "1. **Float32 vs float64**: Pure float32 is faster but can produce QE errors\n",
    "   of several percent. Float64 is the ground truth but 2x slower.\n",
    "\n",
    "2. **Mixed precision eigendecomp**: Promotes only the eigenvalue problem to\n",
    "   float64, achieving float64-level accuracy at near-float32 speed.\n",
    "   This is the recommended default (`precision_strategy: \"mixed\"`).\n",
    "\n",
    "3. **Adaptive fallback**: `AdaptivePrecisionRunner` automatically escalates\n",
    "   precision (f32 -> f64 -> CPU-f64) when energy conservation fails.\n",
    "\n",
    "4. **StabilityDiagnostics**: Pre-simulation checks catch risky configurations\n",
    "   (large matrices with low precision, TF32 enabled, naive factorization).\n",
    "   Post-simulation checks verify QE range and energy conservation.\n",
    "\n",
    "**Best practices:**\n",
    "- Always use `precision_strategy: \"mixed\"` (default)\n",
    "- Always set `allow_tf32: false` for RCWA\n",
    "- Use `fourier_factorization: \"li_inverse\"` for structures with metals\n",
    "- Run `StabilityDiagnostics.pre_simulation_check()` before production sweeps\n",
    "- Validate energy conservation on every result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
